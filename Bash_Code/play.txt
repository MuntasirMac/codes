cat plays_and_poems_stat.csv | csvcut -c 1-3 | head -n 2 | csvlook

head -n 1 plays_and_poems_stat.csv | sed -e 's/,/\'$'\n/g' //This command will convert the first line (all column titles) into a single column

Now, we should be able to grep the text types (play/poems) from the output. For example, if we want to grab the “poems”, we execute the following:

head -n 1 plays_and_poems_stat.csv | sed -e 's/,/\'$'\n/g' | grep "poem"  

Finally, we count the number of lines, which will give us the total number of “poems” in the datasets.

head -n 1 plays_and_poems_stat.csv | sed -e 's/,/\'$'\n/g' | grep "poem" | wc -l

Using the same command, we can also extract the works by “plays”, “Shakespeare” or any other authors in the dataset, easy!

head -n 1 plays_and_poems_stat.csv | sed -e 's/,/\'$'\n/g' | grep "Shakespeare" | wc -l

                        *****Finding plays/poems by each author (sed, sort, uniq, head)*****

Step 1: Convert the first line into a column, $head -n 1 plays_and_poems_stat.csv | sed -e 's/,/\'$'\n/g'
Step 2: For each line, remove all the bits that appeared before the (___play___) and (___poem___), including them. This way we get the raw ‘author names’: sed -r 's/([_a-zA-Z0-9]*)(___play___)|([_a-zA-Z0-9]*)(___poem___)/g'
Step 3: Find the unique appearances of each author sort | uniq -c. Not e that it is a requirement that you need to run sort, before you call the uniq. We use uniq -c, because it will then not only “condense” the neighboring lines that are the same, but also count how many of each are seen!

Now let’s combine all the steps into a piped (|) commmand as the steps need to appear in a tandem:

head -n 1 plays_and_poems_stat.csv | sed -e 's/,/\'$'\n/g' | sed -r 's/([_a-zA-Z0-9]*)(___play___)|([_a-zA-Z0-9]*)(___poem___)//g' | sort | uniq -c

Notice that the OR (|) in the sed statement does not get affected by the pipes (|).

/*head -n 1 plays_and_poems_stat.csv | \
sed -e 's/,/\'$'\n/g' |  \
sed -r 's/([_a-zA-Z0-9]*)(___play___)|([_a-zA-Z0-9]*)(___poem___)//g' | \
sort | uniq -c | sort -n -r -k1*/

We can also add a reverse (-r) numeric sort on the number of works (i.e., the first column), but at the end of the above command. This will definitely make the output more interesting!

1234
head -n 1 plays_and_poems_stat.csv | \sed -e 's/,/\'$'\n/g' |  \sed -r 's/([_a-zA-Z0-9]*)(___play___)|([_a-zA-Z0-9]*)(___poem___)//g' | \sort | uniq -c | sort -n -r -k1


Given a text, what are the most frequent words?

Finding the most frequent words for a given text (e.g., Knight_of_the_Burning_Pestle) is easy, we can build a function toptokens(), which is nothing but the topcrimes() function developed in our previous project. For example, if we want to grab the most frequent words in the Romeo and Juliet play, we can execute the following:

function toptokens() { cat $1 | \
csvcut -c "tokens",$2 | \
sort -nr -t "," -k 2 | \
head -n 20 | \
awk -F',' '{print $1 "," $2}' ; }

toptokens plays_and_poems_stat.csv "Romeo_and_Juliet___play___Shakespeare" |  csvlook

Given an author, what are the most frequent words?

This is slightly complicated! becuase we again need to perform several steps:

For the given author, trim out the plays/ poems names, indclding text types (i.e., plays | poems)
Combine all the columns, i.e., sum horizontally the frequencies of words for all the texts of that author
Sort the words, based on the accumulated frequencies on all works by that author.
Don’t be scared! we will take you there.

Step 1. Trim out the plays/ poems names, for a given author:

Let’s consider that the author in question is Shakespeare. The following awk based regular expression will trim out all the bit before the name of the author. If you look closely, you will see that inside the sed regex, it’s actually finding the pattern of plays OR (|) poems names that end with the string “Shakespeare” and then replacing inplace (due to the -i -r) the whole matched pattern e.g., Romeo_and_Juliet___play___Shakespeare with the string Shakespeare:

123
$ sed -i -r 's/([_a-zA-Z0-9]*)(___play___)(Shakespeare)| \([_a-zA-Z0-9]*)(___poem___)(Shakespeare)/Shakespeare/g' \plays_and_poems_stat.csv

$ sed -i -r 's/([_a-zA-Z0-9]*)(___play___)(Shakespeare)| \
([_a-zA-Z0-9]*)(___poem___)(Shakespeare)/Shakespeare/g' \
plays_and_poems_stat.csv
At this stage, we have a file, where all the Shakespeare works have renamed to “Shakespeare”.

Step 2 Separate all the works of “Shakespeare”

In this step, we build a function (colcut()), which, given the column title (e.g., “Shakespeare”) spit out all the columns with that title including the first column (tokens), which we will write onto a file (Shakepeare.csv). Also note the use of the new command paste, which merges lines of files and writes to standard output lines consisting of sequentially corresponding lines of each given file.

12345
function colcut() { cut -f 1, $(head -1 $1 | sed 's/,/\'$'\n/g' | \grep -n "$2" | \cut -f1 -d: | \paste -sd",") \-d, $1 ; }

function colcut() { cut -f 1, $(head -1 $1 | sed 's/,/\'$'\n/g' | \
grep -n "$2" | \
cut -f1 -d: | \
paste -sd",") \
-d, $1 ; }
We use this function as follows:

1
colcut plays_and_poems_stat.csv Shakespeare > Shakespeare.csv

colcut plays_and_poems_stat.csv Shakespeare > Shakespeare.csv
Note that we can not use csvcut because it can not handle multiple columns with ‘same’ title, which is our case (Shakespeare).

Step 3. Combine/sum horizontally all the columns with same titles (e.g., Shakespeare).

Finally, our final bit of code looks like below. We apply the following awk code to the Shakespeare.csv file which will do the trick for us!

1234567891011
awk -F, '{   TR=0  for( I = 1; I <= NF; I++ ) {    TR += $I    TC[I] += $I    if(I==1)printf( "%6s", $I )  }  print "," TR  TF = NF}' Shakespeare.csv | tail -n +2 | sort -nr -t"," -k2 

awk -F, '{ 
  TR=0
  for( I = 1; I <= NF; I++ ) {
    TR += $I
    TC[I] += $I
    if(I==1)printf( "%6s", $I )
  }
  print "," TR
  TF = NF
}

This small awk code will combine and sum horizontally all the columns (for any number of columns). Note that at the end we again sort the output based on the second column (i.e., combined and summed frequencies).

The final output will look like below:

Note that due to some garbage characters (e.g., page numbers) in the data set, we excluded tokens that are numbers. We only have shown word tokens, using a grep [a-z]' at the end of the command. There we go, the most frequent five words in all Shakespearean works:

the,
and,
I,
of, and
a.

